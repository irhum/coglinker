{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUe1vWra-kaR"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DLEycmIY81jT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# System installs\n",
        "!sudo apt install texlive-extra-utils\n",
        "!sudo apt install tralics\n",
        "\n",
        "# Python installs\n",
        "%mkdir papers\n",
        "%pip install python-magic\n",
        "%pip install latex2mathml\n",
        "%pip install --upgrade pip\n",
        "%pip install openai\n",
        "\n",
        "# Forked version, works without GROBID\n",
        "!git clone https://github.com/irhum/s2orc-doc2json.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV4x8EnB-ser"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vSNmIVk8hM7o"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = 'YOUR KEY HERE' #@param {type:\"string\"}\n",
        "paper_idx = \"2210.05359\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtojh8H7Hzt"
      },
      "source": [
        "### Download paper's LaTeX source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U1j-UOhr9Auu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import functools\n",
        "import tenacity\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "sys.path.append(\"s2orc-doc2json\")\n",
        "from doc2json.tex2json import process_tex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cu12vMU39IUn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "idx_cleaned = paper_idx.replace(\".\", \"_\")\n",
        "!curl https://arxiv.org/e-print/{paper_idx} -o {idx_cleaned}.gz\n",
        "process_tex.process_tex_file(f\"{idx_cleaned}.gz\", output_dir=\"papers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ot7h_g17N4w"
      },
      "source": [
        "### LaTeX to processed text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l9hiuGp_9Jn9"
      },
      "outputs": [],
      "source": [
        "def process_passages(raw_passages):\n",
        "    passages = []\n",
        "\n",
        "    for raw_passage in raw_passages:\n",
        "        # These are strings with \"reference IDs\" for equations/tables/figs\n",
        "        passage = raw_passage['text']\n",
        "\n",
        "        # indicator if passage is a display mode latex passage\n",
        "        is_display = False\n",
        "\n",
        "        # replace equation ref ids with actual latex\n",
        "        for span in raw_passage['eq_spans']:\n",
        "            passage = passage.replace(span['ref_id'], f\"${span['latex']}$\")\n",
        "            if 'DISPLAY' in span['ref_id']:\n",
        "                is_display = True\n",
        "\n",
        "        # if either displaymode latex passage, or a short passage add to preceding\n",
        "        if is_display or (len(passages) > 0 and len(passage) < 150):\n",
        "            passages[-1] = passages[-1] + passage\n",
        "        \n",
        "        # if passage is longer than 1200 chars, break into chunks of 750 chars max.\n",
        "        elif len(passage) > 1200:\n",
        "            MAX_CHUNK = 750\n",
        "            passage_chunks = [passage[i:i + MAX_CHUNK] for i in range(len(passage), MAX_CHUNK)]\n",
        "            passages.extend(passage_chunks)\n",
        "        else:\n",
        "            passages.append(passage)\n",
        "    \n",
        "    return passages\n",
        "\n",
        "# Load processed JSON\n",
        "with open(f\"papers/{idx_cleaned}.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# extract abstract, and list of passages\n",
        "abstract = \"\".join([texts['text'] for texts in data['latex_parse']['abstract']])\n",
        "passages = process_passages(data['latex_parse']['body_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnKqqXtQ9lxL"
      },
      "source": [
        "### Embed passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LFnzg2AM5xi5"
      },
      "outputs": [],
      "source": [
        "@tenacity.retry(wait=tenacity.wait_random_exponential(min=1, max=60))\n",
        "def get_embed(text, model=\"text-embedding-ada-002\"):\n",
        "   response = openai.Embedding.create(input = [text], model=model)\n",
        "   return response['data'][0]['embedding']\n",
        "\n",
        "embeds_resp = [get_embed(passage) for passage in passages]\n",
        "embeds = np.array(embeds_resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Odkll6g-Zw"
      },
      "source": [
        "## Q/A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fID0lpqvg9l8"
      },
      "outputs": [],
      "source": [
        "question = \"How is the physics sim integrated with the language model?\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_HgLj4x9y2m"
      },
      "source": [
        "### Step 1: Generate \"hypothetical passages\" to answer the question.\n",
        "\n",
        "This is inspired by the [HyDE](https://arxiv.org/abs/2212.10496) paper, which makes the following discovery: the embedding of a real answer is closer to a \"hypothetical answer\" than it is to the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4wNgWzZ0r-pu"
      },
      "outputs": [],
      "source": [
        "messages_gen = [{\"role\": \"system\", \"content\": r\"\"\"You are an interactive academic paper writing assistant. You are well-versed in authoritative, academic writing, and produce perfect LaTeX where needed to support your idea. The user interacts with you as follows:\n",
        "* The user provides you an abstract.\n",
        "* Once you finish analyzing, you prompt the user for a question.\n",
        "* The user asks you a question. In response, you produce a passage (Passage 1).\n",
        "    * This should NOT directly answer the question; it is a passage from the final paper you two are writing.\n",
        "* You also produce an alternate response (Passage 2), which is about some small technical detail in Passage 1\"\"\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": \"Here's the abstract for the paper we're looking at: Likelihood, although useful as a training loss, is a poor search objective for guiding open-ended generation from language models (LMs). Existing generation algorithms must avoid both unlikely strings, which are incoherent, and highly likely ones, which are short and repetitive. We propose contrastive decoding (CD), a more reliable search objective that returns the difference between likelihood under a large LM (called the expert, e.g. OPT-13b) and a small LM (called the amateur, e.g. OPT-125m). CD is inspired by the fact that the failures of larger LMs are even more prevalent in smaller LMs, and that this difference signals exactly which texts should be preferred. CD requires zero training, and produces higher quality text than decoding from the larger LM alone. It also generalizes across model types (OPT and GPT2) and significantly outperforms four strong decoding algorithms in automatic and human evaluations. FOOTREF1\"},\n",
        "\n",
        "{\"role\": \"assistant\", \"content\": \"I've finished analyzing this abstract. How can I help?\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": \"How does this technique choose the next best token?\"},\n",
        "\n",
        "{\"role\": \"assistant\", \"content\": \"\"\"Passage 1 - [[In order to contrast the expert and amateur LM, contrastive decoding searches for text that maximizes the contrastive objective $\\mathcal {L}_{\\text{CD}} = \\log p_\\textsc {exp}(\\textsf {x$ cont $}\\mid \\textsf {x$ pre $}) - \\log p_\\textsc {ama}(\\textsf {x$ cont $}\\mid \\textsf {x$ pre $})$ , subject to constraints that $\\textsf {x$ cont $}$ should be plausible (i.e., achieve sufficiently high probability under the expert LM)]].\n",
        "Passage 2 - [[As shown in Equation EQREF5 , we first filter tokens based on plausibility constraints $\\mathcal {V}_\\text{head}(x_{<i})$ , eliminating tokens that fail to achieve sufficiently high probabilities under the expert LM. Then we score the remaining tokens based on the amount of contrast they demonstrate, according to $ \\log p_\\textsc {exp}( x_i \\mid x_{<i}) - \\log p_\\textsc {ama}( x_i \\mid x_{<i})$ . As a result, we end up selecting plausible tokens under the expert LM that least resemble the amateur LM.]]\"\"\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": f\"Great work! Let's move on to a different topic. Here's the abstract for a new paper we're looking at: {abstract}\"},\n",
        " \n",
        "{\"role\": \"assistant\", \"content\": \"How can I help?\"},\n",
        " \n",
        "{\"role\": \"user\", \"content\": f\"{question}\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5ywkaY4aSJmp"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',  \n",
        "    temperature=0.8,            \n",
        "    messages=messages_gen, \n",
        ")\n",
        "\n",
        "gen_passages = response['choices'][0]['message']['content']\n",
        "gen_passages = [text[2:-2] for text in re.findall(r'\\[\\[.*\\]\\]', gen_passages)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prC9NTkt-Pke"
      },
      "source": [
        "### Step 2: Retrieve best \"real\" passage\n",
        "\n",
        "We embed the \"hypothetical\" passage, and use maximum inner product search (implemented as a literal inner product, since we're doing a 1 to ~100 vectors search) to find a \"real\" passage that's most similar.\n",
        "\n",
        "Note we generate two \"hypothetical\" passages but only use the second one. This isn't for super \"necessary\" reasons; the current prompt just means on average it has more \"technical content\", and is more likely to match the actual desired passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k5DOg2qT-GGy"
      },
      "outputs": [],
      "source": [
        "gen_passages_emb = np.array([get_embed(gen_passage) for gen_passage in gen_passages])\n",
        "retr_passages = [passages[i] for i in np.argsort(-embeds @ gen_passages_emb[1])][:4]\n",
        "retr_passages = \"\\n\".join([f'Passage {i} - <<{passage}>>' for (i, passage) in enumerate(retr_passages)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bowzG8yB_Iaw"
      },
      "source": [
        "### Step 3: Answer generation\n",
        "\n",
        "We retrieve the four closest \"real\" passages to the \"hypothetical\" passage, and then inject them into the prompt. And then, just ask for the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EkjoFSYv5r2E"
      },
      "outputs": [],
      "source": [
        "messages_ans = [{\"role\": \"system\", \"content\": \"\"\"You are a research assistant with access to a powerful search engine for scientific contexts. When your user asks you a question, you:\n",
        "1. Retrieve up to 4 relevant passages. Although these passages may have grammatical/syntax errors, they are factually correct.\n",
        "2. Analyze the relevance of the passages to the question being asked.\n",
        "3. Produce a response that is as concise as possible while answering the question.\"\"\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": \"Here's the abstract for the paper we're looking at: Likelihood, although useful as a training loss, is a poor search objective for guiding open-ended generation from language models (LMs). Existing generation algorithms must avoid both unlikely strings, which are incoherent, and highly likely ones, which are short and repetitive. We propose contrastive decoding (CD), a more reliable search objective that returns the difference between likelihood under a large LM (called the expert, e.g. OPT-13b) and a small LM (called the amateur, e.g. OPT-125m). CD is inspired by the fact that the failures of larger LMs are even more prevalent in smaller LMs, and that this difference signals exactly which texts should be preferred. CD requires zero training, and produces higher quality text than decoding from the larger LM alone. It also generalizes across model types (OPT and GPT2) and significantly outperforms four strong decoding algorithms in automatic and human evaluations. FOOTREF1\"},\n",
        "\n",
        "{\"role\": \"assistant\", \"content\": \"I've finished analyzing this abstract. How can I help?\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": \"Retrieve upto 4 high quality passages to answer the following: How does this technique choose the next best token?\"},\n",
        "\n",
        "{\"role\": \"assistant\", \"content\": \"\"\"Passage 1 - <<In order to contrast the expert and amateur LM, contrastive decoding searches for text that maximizes the contrastive objective $\\mathcal {L}_{\\text{CD}} = \\log p_\\textsc {exp}(\\textsf {x$ cont $}\\mid \\textsf {x$ pre $}) - \\log p_\\textsc {ama}(\\textsf {x$ cont $}\\mid \\textsf {x$ pre $})$ , subject to constraints that $\\textsf {x$ cont $}$ should be plausible (i.e., achieve sufficiently high probability under the expert LM).>>\n",
        "Passage 2 - <<As shown in Equation EQREF5 , we first filter tokens based on plausibility constraints $\\mathcal {V}_\\text{head}(x_{<i})$ , eliminating tokens that fail to achieve sufficiently high probabilities under the expert LM. Then we score the remaining tokens based on the amount of contrast they demonstrate, according to $ \\log p_\\textsc {exp}( x_i \\mid x_{<i}) - \\log p_\\textsc {ama}( x_i \\mid x_{<i})$ . As a result, we end up selecting plausible tokens under the expert LM that least resemble the amateur LM.>>\"\"\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": \"Using these passages (directly copying LaTeX as needed), answer the question.\"},\n",
        "\n",
        "{\"role\": \"assistant\", \"content\": \"First, we filter out all tokens that the expert LM doesn't assign high probabilities to. Then, each token is scored based on the *difference* in log-likelihood between the expert and amateur LMs ($ \\log p_\\textsc {exp}( x_i \\mid x_{<i}) - \\log p_\\textsc {ama}( x_i \\mid x_{<i})$). In doing so, we take advantage of the fact that the amateur LM provides a signal of which tokens to *avoid*.\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": f\"Let's move on to a new topic. Here's the abstract for the new paper we're looking at: {abstract}\"},\n",
        " \n",
        "{\"role\": \"assistant\", \"content\": \"Okay, what's your question?\"},\n",
        " \n",
        "{\"role\": \"user\", \"content\": f\"Retrieve upto 4 high quality passages to answer the following: {question}\"},\n",
        "\n",
        "{\"role\": \"assistant\", \"content\": f\"{retr_passages}\"},\n",
        "\n",
        "{\"role\": \"user\", \"content\": \"Analyzing and extracting information from these passages, answer the question. DO NOT make unsupported claims.\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wvOLe5jzhB-p"
      },
      "outputs": [],
      "source": [
        "completions = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',  \n",
        "    temperature=0.5,\n",
        "    messages=messages_ans\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzquzEN6_pHi"
      },
      "source": [
        "### Get answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "nQW7LSF3_rMF",
        "outputId": "3efdee15-6abf-4c9f-e1ab-82006d2d5ddd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The physics simulation engine (MuJoCo) is integrated with the language model (LM) in Mind's Eye by appending the simulation results to the input prompts of LMs during inference. The simulation engine returns the most likely outcome based on its encoded world knowledge, which is then used as part of the input for the LM. This allows the LM to perform grounded reasoning in the physical world. The implementation of Mind's Eye comprises three main components: a text-to-code LM as the front-end, a physics simulation engine as the back-end, and a foundation model for general reasoning. The proposed method can be used as a plug-and-play framework that works with any LM and requires neither handcrafted prompts nor costly fine-tuning.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completions['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brLAm00gJW0w"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "### Tech\n",
        "#### Information Retrieval (IR)\n",
        "* Just embedding the question as the query vector does *not* work well.\n",
        "* Generate \"hypothetical\" answers and embedding them works...if the answers are relevant. Need better prompts to actually encourage the model to hallucinate.\n",
        "* ^This generation process feels sledge-hammery though. There's only like 100 vectors per PDF, can't we just use BM25? \n",
        "\n",
        "#### Synthesis\n",
        "* In early tests, if I provide \"gold standard\" retrieved passages, ChatGPT seems quite good at condensing them into an answer. The problem then is in the IR, not the LM itself.\n",
        "\n",
        "### Usefulness\n",
        "* This is not a useful tool as designed. If I want to know if there's a scaling law plot in the paper...I can just scan the paper, no need to wait 20 seconds for this whole pipe to execute.\n",
        "* More interesting is stuff *across* papers. What if I'm reading through this *application* of the [GBP algorithm to robots](https://arxiv.org/abs/2203.11618), and behind the scenes the system also scrapes [this](https://arxiv.org/abs/2107.02308) so I can ask questions about GBP itself?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PUe1vWra-kaR",
        "lDtojh8H7Hzt",
        "7Ot7h_g17N4w",
        "lnKqqXtQ9lxL",
        "2_HgLj4x9y2m",
        "prC9NTkt-Pke",
        "bowzG8yB_Iaw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
